---
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figs/",
  fig.height = 3,
  fig.width = 4,
  fig.align = "center"
)
```

[\@drsimonj](https://twitter.com/drsimonj) here to discuss how to conduct k-fold cross validation, with an emphasis on evaluating models supported by [David Robinson's](http://varianceexplained.org/) [broom](https://cran.r-project.org/web/packages/broom/index.html) package. Full credit also goes to David, as this is a slightly more detailed version of [his past post](http://rpubs.com/dgrtwo/cv-modelr), which I read some time ago and felt like unpacking.

## Assumed knowledge: K-fold Cross validation

This post assumes you know what k-fold cross validation is. If you want to brush up, here's a [fantastic tutorial from  Stanford University professors Trevor Hastie and Rob Tibshirani](https://www.youtube.com/watch?v=nZAM5OXrktY).

## Creating folds

Before worrying about models, we can generate K folds using `crossv_kfold` from the [modelr](https://cran.r-project.org/web/packages/modelr/index.html) package. Let's practice with the `mtcars` data to keep things simple.

```{r}
library(modelr)
set.seed(1)  # Run to replicate this post
folds <- crossv_kfold(mtcars, k = 5)
folds
```

This function takes a data frame and randomly partitions it's rows (1 to `r nrow(mtcars)` for `mtcars`) into `k` roughly equal groups. We've partitioned the row numbers into `k = 5` groups. The results are returned as a tibble (data frame) like the one above.

Each cell in the `test` column contains a `resample` object, which is an efficient way of referencing a subset of rows in a data frame (`?resample` to learn more). Think of each cell as a reference to the rows of the data frame belonging to each partition. For example, the following tells us that the first partition of the data references rows *`r folds$test[[1]]$idx`*, which accounts for roughly `1 / k` of the total data set (`r length(folds$test[[1]]$idx)` of the `r nrow(mtcars)` rows).

```{r}
folds$test[[1]]
```

Each cell in `train` also contains a `resample` object, but referencing the rows in all other partitions. For example, the first `train` object references all rows **except** *`r folds$test[[1]]$idx`*:

```{r}
folds$train[[1]]
```

We can now run a model on the data referenced by each `train` object, and validate the model results on each corresponding partition in `test`.

## Fitting models to training data

Say we're interested in predicting Miles Per Gallon (`mpg`) with all other variables. With the whole data set, we'd do this via:

```{r, eval = FALSE}
lm(mpg ~ ., data = mtcars)
```

Instead, we want to run this model on each set of training data (data referenced in each `train` cell). We can do this as follows:

```{r, message = F}
library(dplyr)
library(purrr)

folds <- folds %>% mutate(model = map(train, ~ lm(mpg ~ ., data = .)))
folds
```

- `folds %>% mutate(model = ...)` is adding a new `model` column to the folds tibble.
- `map(train, ...)` is applying a function to each of the cells in `train`
- `~ lm(...)` is the regression model applied to each `train` cell.
- `data = .` specifies that the data for the regression model will be the data referenced by each `train` object.

The result is a new `model` column containing fitted regression models based on each of the `train` data (i.e., the whole data set excluding each partition).

For example, the model fitted to our first set of training data is:

```{r}
folds$model[[1]] %>% summary()
```

## Predicting the test data

The next step is to use each model for predicting the outcome variable in the corresponding `test` data. There are many ways to achieve this. One general approach might be:

```{r, eval = FALSE}
folds %>% mutate(predicted = map2(model, test, <FUNCTION_TO_PREDICT_TEST_DATA> ))
```

`map2(model, test, ...)` iterates through each model and set of `test` data in parallel. By referencing these in the function for predicting the test data, this would add a `predicted` column with the predicted results.

For many common models, an elegant alternative is to use `augment` from [broom](https://cran.r-project.org/web/packages/broom/index.html). For regression, `augment` will take a fitted model and a new data frame, and return a data frame of the predicted results, which is what we want! Following above, we can use `augment` as follows:

```{r, message = F}
library(broom)

folds %>% mutate(predicted = map2(model, test, ~ augment(.x, newdata = .y)))
```

To extract the relevant information from these `predicted` results, we'll `unnest` the data frames thanks to the [tidyr](https://cran.r-project.org/web/packages/tidyr/index.html) package:

```{r, message = F}
library(tidyr)

folds %>%
  mutate(predicted = map2(model, test, ~ augment(.x, newdata = .y))) %>% 
  unnest(predicted)
```

This was to show you the intermediate steps. In practice we can skip the `mutate` step:

```{r}
predicted <- folds %>% unnest(map2(model, test, ~ augment(.x, newdata = .y)))
predicted
```

We now have a tibble of the `test` data for each fold (`.id` = fold number) and the corresponding `.fitted`, or predicted values for the outcome variable (`mpg`) in each case.

## Validating the model

We can compute and examine the residuals:

```{r}
# Compute the residuals
predicted <- predicted %>% 
  mutate(residual = .fitted - mpg)

# Plot actual v residual values
# coloured by fold
library(ggplot2)
predicted %>%
  ggplot(aes(mpg, residual)) +
    geom_hline(yintercept = 0) +
    geom_point() +
    stat_smooth(method = "loess") +
    theme_minimal()
```

It looks like our models could be overestimating `mpg` around 20-30 and underestimating higher `mpg`. But there are clearly fewer data points, making prediction difficult.

We can also use these values to calculate the overall proportion of variance accounted for by each model:

```{r}
rs <- predicted %>%
  group_by(.id) %>% 
  summarise(
    sst = sum((mpg - mean(mpg)) ^ 2), # Sum of Squares Total
    sse = sum(residual ^ 2),          # Sum of Squares Residual/Error
    r.squared = 1 - sse / sst         # Proportion of variance accounted for
    )
rs

# Overall
mean(rs$r.squared)
```

So, across the folds, the regression models have accounted for an average of `r round(mean(rs$r.squared) * 100, 2)`% of the variance of `mpg` in new, test data.

Plotting these results:

```{r, message = F}
rs %>% 
  ggplot(aes(r.squared, fill  = .id)) +
    geom_histogram() +
    geom_vline(aes(xintercept = mean(r.squared)))  # Overall mean
```

Although the model performed well on average, it performed pretty poorly when fold 1 was used as test data.

## All at once

With this new knowledge, we can do something similar to the `k = 20` case shown in [David's post](http://rpubs.com/dgrtwo/cv-modelr). See that you can understand what's going on here:

```{r}
set.seed(1)
# Select four variables from the mpg data set in ggplot2
ggplot2::mpg %>% select(year, cyl, drv, hwy) %>% 
  # Create 20 folds (5% of the data in each partition)
  crossv_kfold(k = 20) %>%
  # Fit a model to training data
  mutate(model = map(train, ~ lm(hwy ~ ., data = .))) %>%
  # Unnest predicted values on test data
  unnest(map2(model, test, ~ augment(.x, newdata = .y))) %>% 
  # Compute R-squared values for each partition
  group_by(.id) %>%
  summarise(
    sst = sum((hwy - mean(hwy)) ^ 2),
    sse = sum((hwy - .fitted) ^ 2),
    r.squared = 1 - sse / sst
  ) %>% 
  # Plot
  ggplot(aes(r.squared)) +
    geom_density() +
    geom_vline(aes(xintercept = mean(r.squared))) +
    theme_minimal()
```

## Sign off

Thanks for reading and I hope this was useful for you.

For updates of recent blog posts, follow [\@drsimonj](https://twitter.com/drsimonj) on Twitter, or email me at <drsimonjackson@gmail.com> to get in touch.

If you'd like the code that produced this blog, check out the [blogR GitHub repository](https://github.com/drsimonj/blogR).